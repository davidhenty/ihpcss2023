\documentclass{article}

\usepackage{epcc}

\begin{document}

\title{Advanced Message-Passing Programming Exercises \\
	MPI Performance}

\author{David Henty}
\date{}
\makeEPCCtitle

\section{Introduction}

The purpose of this exercise is to investigate the performance of
basic point-to-point MPI operations. You are given a simple ping-pong
code that exchanges messages of increasing size between two MPI ranks,
and prints out the average time taken and the bandwidth over a large
number of repetitions. With this code you can investigate how the
following factors affect the performance:

\begin{itemize}

\item different underlying protocols for different message sizes;
\item different communication hardware (e.g. over the inter-node network or
  intra-node memory copies);
\item different MPI send modes (e.g. synchronous vs. buffered);
\item different MPI datatypes (e.g. contiguous or strided data);
\item different NUMA regions within a node.

\end{itemize}

\section{Compiling and Running}

Check out the IHPCSS23 repo \texttt{git clone https://github.com/davidhenty/ihpcss2023}. 

The code is contained in \verb+AMPP-pingpong.tar+ on the course web
pages. You should be able to compile it using \verb+make+, and submit
the supplied Slurm scripts unchanged. As distributed, the Makefile is
set up for Bridges2.

On Bridges2, you will need to load a non-default module:
\begin{verbatim}
  [user@bridges2 ~]$ module load openmpi
\end{verbatim}

You should compile the code with \texttt{make} and submit using \texttt{sbatch bridges2.job}.

By default, the code runs on two processes each placed on the same
node (so communications will {\bf not} take place over the
network), and benchmarks standard and synchronous sends. Note that
{\bf the program also prints out the exact location of the two
  processes} -- this will enable you to check, for example, whether the two processes
are on the same or different nodes which is important as the message will only
go over the network if you run on different nodes (although the performance
within a node is interesting as it should be much faster).

For each mode, two ``.plot'' files are written containing the times and
the bandwidths as a function of message size. The results for the two
different modes can be compared using:

\begin{verbatim}
  gnuplot -persist plot_time.gp
  gnuplot -persist plot_bandwidth.gp
\end{verbatim}

Check that you understand the general form of the graphs before
proceeding.

\section{Experiments}

The supplied gnuplot ``.gp'' files compare the results for standard and
synchronous modes. If you want to compare different modes, or more than
two modes, you will have to edit the gnuplot files -- the format should
be self-explanatory. Also note that the code overwrites the ``.plot''
files so you will need to make copies after each run if you want to keep
a record of the results (e.g. when changing the placement of processes
on the cores or nodes).

{\bf If you run on more than two processes, the program sends messages
  between the first (rank zero) and last ranks with all the other
  processes remaining idle.} This is useful when altering the
assignment of processes to cores.

By varying \verb+nodes+ and \verb+tasks-per-node+ in the Slurm script
you should have complete control of the placement of the first and
last processes.

After having run the default setup -- two processes on cores 0 and 1
-- you should experiment with the following:

\begin{enumerate}

\item Send between different nodes, e.g. run with a total of two
  processes with one process on each of two nodes.

\item Communication between two processes widely separated on the same
  node (e.g.  \verb+nodes=1+ and \verb+tasks-per-node=128+).

\item Can you explain any variations in the latency and bandwidth
  as you change the process locations?

\item Investigate the performance of buffered or strided sends.

\end{enumerate}

\end{document} 
